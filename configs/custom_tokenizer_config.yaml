# Custom Tokenizer Algorithm Configuration
# This configuration demonstrates how to use a custom algorithm in the LMPipeline

# Base model configuration
model_name_or_path: "microsoft/DialoGPT-small"
output_dir: "./outputs/custom_tokenizer_run"

# Pipeline stages - using only the custom tokenizer algorithm
stages:
  - "custom_tokenizer"

# Stage-specific configurations
stage_configs:
  custom_tokenizer:
    # Tokenizer training parameters
    vocab_size: 16000
    training_corpus_path: "data/domain_specific_corpus.txt"
    tokenizer_type: "bpe"  # Options: bpe, wordpiece, unigram
    min_frequency: 2
    
    # Special tokens to preserve
    special_tokens:
      - "<pad>"
      - "<unk>"
      - "<s>"
      - "</s>"
      - "<domain>"
      - "<technical>"
    
    # Base stage configuration
    enabled: true
    save_intermediate: true
    load_best_model: true
    
    # Logging and monitoring
    use_wandb: true
    wandb_project: "custom-tokenizer-experiments"
    wandb_run_name: "domain-specific-tokenizer-v1"

# Global pipeline settings
save_final_model: true
cleanup_intermediate: false
log_level: "INFO"

# Optional: Hub upload configuration
push_to_hub: false
hub_repo_id: "your-username/custom-tokenizer-model"
hub_commit_message: "Custom tokenizer trained on domain-specific corpus"
hub_private: true

# Optional: Post-processing
convert_to_gguf: false
gguf_output_path: "./outputs/custom_tokenizer_run/model.gguf"
